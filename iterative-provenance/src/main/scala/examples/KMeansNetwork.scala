package examples

import org.apache.spark.SparkConf
import org.apache.spark.lineage.LineageContext
import org.apache.spark.lineage.LineageContext._
import org.apache.spark.lineage.rdd.LineageRDD
import org.apache.spark.sql.SparkSession

// jteoh: Mix of gulzar's work, ml.KMeansExample, and gist @ https://gist.github.com/umbertogriffo/b599aa9b9a156bb1e8775c8cdbfb688a
object KMeansNetwork {
  
  def parseVector(line: String): Array[Double] =
    line.split(" ").map(_.toDouble)
  
  def parseAutogeneratedVector(line: String): Array[Double] =
    // ex: 0 1:313.37 2:-307.07 3:651.23 4:728.55 5:-254.32 6:676.80 7:-136.03 8:1875.63 9:-859.88
    // format: rowID, colId:value
    line.split(" ").drop(1).map(_.split(":")(1).toDouble)
  
  def squaredDistance(p: Array[Double],
                      center: Array[Double]): Double= {
    p.zip(center).map(s => s._1 - s._2).map(s => s * s).sum
  }
  
  def closestCenter(p: Array[Double],
                    centers: Array[Array[Double]]): Int = {
    centers.indices.minBy(i => squaredDistance(p, centers(i)))
    /**
    var bestIndex = 0
    var closest = Double.PositiveInfinity
    
    for (i <- centers.indices) {
      val tempDist = squaredDistance(p, centers(i))
      if (tempDist < closest) {
        closest = tempDist
        bestIndex = i
      }
    }
    bestIndex
     **/
  }
  
  // args: dataset, number of clusters, convergence distance, maxIterations, seed
  // defaults: (local file), 3,
  def main(args: Array[String]): Unit = {
    
    val conf = new SparkConf().setMaster("local[*]")
      // .set("spark.eventLog.dir", "/tmp/spark-events")
                              .set("spark.eventLog.enabled", "true")
    val spark = SparkSession.builder
                            .config(conf)
                            .appName("SparkKMeans")
                            .getOrCreate()
    val sc = spark.sparkContext
    sc.setLogLevel("WARN")
    val lc = new LineageContext(sc)
    // TODO JTEOH setting capture lineage
    lc.setCaptureLineage(true) // make sure this doesn't overlap with the later case
  
    val useDatasetSchema = args.headOption.filter(_.nonEmpty).getOrElse("generated")
    val data = useDatasetSchema match {
      case "generated" =>
        val lines = lc.textFile("/Users/jteoh/Code/FineGrainedDataProvenance/KMeans_1M_3")
        //"/Users/malig/workspace/git/ADSpark/src/main/resources/delivery_data.txt"
        lines.map(s => parseAutogeneratedVector(s)).cache() // careful: caching will break later rbk
      case "spark-default" =>
        val lines = lc.textFile("/Users/jteoh/Code/distributions/spark-2.2.0-bin-hadoop2.7/data/mllib/kmeans_data.txt")
        //"/Users/malig/workspace/git/ADSpark/src/main/resources/delivery_data.txt"
        lines.map(s => parseVector(s)).cache() // careful: caching will break later rbk
      // because of double-computation.
      case "unknown" =>
        // local testing only, data format is different...
        // this is a larger dataset to test with.
        lc.textFile(args.headOption
                        .getOrElse("/Users/jteoh/Code/FineGrainedDataProvenance/kmeans_sample_data"))
          .map(line => line.split(','))
          .map(pair => Array(pair(3).toDouble, pair(4).toDouble))
          .filter(point => !point.contains(0.0))
    }
    data.setName("data")
  
  
    // TODO: data should be persisted, but then taps won't work later in reduceByKey
    // data.persist()
  
    val K = args.lift(1).map(_.toInt).getOrElse(3) // number of clusters
    val convergeDist = args.lift(2).map(_.toDouble).getOrElse(0.1) // based on gist
    val maxIter = args.lift(3).map(_.toInt).getOrElse(20) // 20 iterations based on ml.KMeans
    val seed = args.lift(4).map(_.toInt).getOrElse(42)
  
    val kPoints: Array[Array[Double]] = data.takeSample(withReplacement = false, K, seed)
    //val kPoints: Array[Array[Double]] = Array(Array(1.0, 2.0), Array(2.0, 3.0), Array(3.0, 4.0))
    var tempDist = Double.PositiveInfinity // used to measure convergence rate
    var iteration = 1
  
  
    // Key differences between simple and library implementation:
    // (1) Initialization options - uses simple random as opposed to optimized version. Should be fine for our use case.
    // (2) broadcasting the centers rather than serializing tasks (should be negligible for small K)
    // (3) mapPartitions to optimize the cluster-sum+count outputs, as opposed to relying solely on reduceByKey
  
    // Detailed description of how it works in distributed fashion:
    // (0) initialize center points (typically done with KMeansParallel approach, but we use random for simplicity)
    // each iteration (until max iterations or convergence as defined in 7.1)
    // (1) broadcast the center points
    // (2) create a costAccumulator (double) - (this isn't strictly required though, just to measure accuracy of result)
    // (3) map data per partition:
    // (3.0): initialize (a) sum-vectors for each center, and (b) count for each center.
    // (3.1) for each point, find its closest center and the cost (distance)
    // (3.2) add cost to the cost costAccumulator
    // (3.3) for the closest center: add to sum (using axpy) and increment count (from 3.0)
    //    y += a*x, with a = 1, x = point-vector, y = sum (in other words, sum the vectors)
    // (3.4) produce (center_index, (sum_index, count_index)) for each center with non-zero count
    // (4) reduceByKey: for each center: add the sum-vectors and the counts together.
    // (5) collectAsMap: these will be used to later calculate the new centers
    // (6) delete broadcast variable from (1)
    // (7) update the collected map to compute centers (sum-vector / scalar count)
    // (7.1) checking if distance between old and new center is within epsilon^2 (convergence)
    // (8) Update cost (from accumulator) and iteration counter
    // (8.1) cost is actually never really used, except for one print at the end of all execution
  
    // JTEOH
    // lc.setCaptureLineage(true)
  
    val iterationStartData = data.markIterationStart()
    //val preIterationLineage = iterationStartData.getPreIterationLineage()
    //println("PreIteration Count: " + preIterationLineage.count())
  
    // count data to... do something.
    //println("Data count: " + data.count())
  
    var iterationMergedLineage: LineageRDD = null
    while (tempDist > convergeDist && iteration <= maxIter) {
      // TODO: need to unpersist so lineage is computed for data each round
      // TODO Friday: Getting error: Int cannot be cast to Text, after the TapHadoopLRDD
      // I believe this is because the TapHadoopLRDD is being cached and returning lineage results
      // instead of original hadoopFile results.
      // We need a way to indicate inputs to an iteration and assign them their own unique
      // iteration-based input IDs.
      // typically for an entire job we just generate new IDs - this would work as well...?
      // See; TapHadoopLRDD, TapPost__
      //
      // data.unpersist(true) // Not quite right...
      lc.setCaptureLineage(true) // since we disable it at the end of each iteration...
    
      // find the closest center for each point. also print out (point, 1) similar to wordcount...
      val closest = iterationStartData.map(p => (closestCenter(p, kPoints), (p, 1)))
                                      .setName(s"Closest @ iter $iteration")
    
      // for each center, sum the points and the counts so we can average them later.
      val pointStats = closest.reduceByKey {
        case ((p1, c1), (p2, c2)) =>
          (p1.zip(p2).map(s => s._1 + s._2), c1 + c2)
      }.setName(s"PointStats @ iter $iteration")
    
      // compute the average point, done by dividing the summed point (vector) by count.
      val newPointsRDD = pointStats
        .map { pair =>
          (pair._1, pair._2._1.map(s => s / pair._2._2))
        }.setName(s"newPoints @ iter $iteration")
    
      val newPoints = newPointsRDD.collectAsMap()
    
      // TODO Thursday: investigate
      val newPointsLineage = newPointsRDD.getIterationLineage()
      // println("Before set capture to false")
      // println(newPointsLineage.toDebugString)
      // println("After set capture to false")
      lc.setCaptureLineage(false)
      // println(newPointsLineage.toDebugString)
    
      // println("NewPoints counts: " + newPointsLineage.count())
      //    println("NewPoints: " + newPointsLineage.collectWithId().foreach(println))
      //val iterationStartLineage = newPointsLineage.goBackUntilIterationStart()
      val iterationStartLineage = newPointsLineage.goBackUntilIterationStart()
      //println("Iteration start? \n" + iterationStartLineage.toDebugString)
    
      /** *** UPDATE ITERATION LINEAGE ****/
      if (iterationMergedLineage == null) {
        iterationMergedLineage = iterationStartLineage.cache()
      } else {
        val before = iterationMergedLineage
        iterationMergedLineage = iterationMergedLineage.union(iterationStartLineage).distinct()
                                                       .cache()
        iterationStartLineage.unpersist()
        before.unpersist()
      }
      // need to force a count or some other computation to ensure this is computed and saved.
      println(s"Iteration ${iteration} lineage counts: ${iterationStartLineage.count()}")
      println(s"Iteration ${iteration} aggregated/merged lineage counts: " +
                s"${iterationMergedLineage.count()}")
    
      /** ********/
    
      //println("IterationLineage: " + iterationStartLineage.collectWithId().foreach(println))
      //println("GoBackAll counts: " + newPointsLineage.goBackAll().count()) //431857
      println("New centers: ")
      println(newPoints.mapValues(_.mkString("[", ",", "]")).mkString(","))
      // data.unpersist(blocking = true)
      // println("All data counts: " + data.count()) //431857
      //.collect().foreach(println)
      //    System.exit(1)
    
    
      // Calculate the rate of convergence (how much our centers moved since previous iteration)
      tempDist = 0.0
      for (i <- 0 until K) {
        tempDist = tempDist + squaredDistance(kPoints(i), newPoints(i))
      }
    
    
      // Update our centers with the newly calculated ones.
      for (newP <- newPoints) {
        kPoints(newP._1) = newP._2
      }
    
      println(s"Finished iteration $iteration (delta = $tempDist)")
      iteration += 1
    
    
    }
  
    println("Final centers:")
    println("====")
  
    kPoints.zipWithIndex.foreach({ case (center, index) => {
      println(index + " " + center.mkString("[", ",", "]"))
      println("===")
    }
    })
  
    println("Iteration merge size: " + iterationMergedLineage.count())
  
    val preIteration = iterationStartData.iterationInput
    // TODO reset lineage/caches/unpersist EVERYTHING /o/
    lc.setCaptureLineage(true)
    println(s"Test: ${preIteration.count()}") // force computation and lineage collection
    val preIterationLineage: LineageRDD = preIteration.getLineage()
    //  println("PRE-ITERATION")
    //  println(preIteration.toDebugString)
    //  println("PRE-ITERATION LINEAGE")
    //  println(preIterationLineage.toDebugString) // currently showing multiple TapLRDDs - red flag.
    lc.setCaptureLineage(false)
    //println("Pre-Iteration lineage count: " + preIterationLineage.count())
    //preIterationLineage.collect().take(5).foreach(println)
  
    //println("GoBackAll: " + preIterationLineage.goBackAll().count())
  
    val iterationFilteredLineage = iterationMergedLineage.filterMerge(preIterationLineage)
    println("Count after combining iteration lineage with pre-iter: " +
              s"${iterationFilteredLineage.count()}")
    val inputIDs: LineageRDD = iterationFilteredLineage.goBackAll()
    println(s"Count of traced inputs: ${inputIDs.count()}")
    //println("IDs")
    //inputIDs.take(10).foreach(println)
    
    // This isn't working for unknown reasons
    //println("Show values")
    // Tried swapping because the left-side seemed to only have partition 0 IDs??)
    // not sure if that's right though, might be an assumption of partitioning under the covers
    // since the actual LHS of the RecordID is not used in showrdd/3-wayjoin
    //inputIDs.map(_.asInstanceOf[(RecordId, RecordId)].swap).show().take(10).foreach(println)
  
  
    spark.stop()
  }
}
